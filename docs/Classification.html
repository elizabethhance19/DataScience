<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Classification</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Clustering.html">Clustering</a>
</li>
<li>
  <a href="Classification.html">Classification</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Classification</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#overview"><span class="toc-section-number">1</span> Overview</a></li>
<li><a href="#logistic-regression"><span class="toc-section-number">2</span> Logistic Regression</a></li>
<li><a href="#k-nearest-neighbors-knn"><span class="toc-section-number">3</span> K-Nearest Neighbors (KNN)</a><ul>
<li><a href="#knn-iris-example"><span class="toc-section-number">3.1</span> KNN: Iris Example</a></li>
</ul></li>
<li><a href="#naive-bayes"><span class="toc-section-number">4</span> Naive Bayes</a><ul>
<li><a href="#naive-bayes-classifier"><span class="toc-section-number">4.1</span> Naive Bayes Classifier</a></li>
<li><a href="#naive-bayes-titanic-example"><span class="toc-section-number">4.2</span> Naive Bayes: Titanic Example</a></li>
<li><a href="#laplace-correction"><span class="toc-section-number">4.3</span> Laplace Correction</a></li>
</ul></li>
<li><a href="#classification-trees"><span class="toc-section-number">5</span> Classification Trees</a></li>
</ul>
</div>

<div id="overview" class="section level1">
<h1><span class="header-section-number">1</span> Overview</h1>
<p>Add knn, naive bayes, logistic regression, classification trees</p>
</div>
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">2</span> Logistic Regression</h1>
</div>
<div id="k-nearest-neighbors-knn" class="section level1">
<h1><span class="header-section-number">3</span> K-Nearest Neighbors (KNN)</h1>
<p>KNN works by considering the K training data points that are closest (typically using Euclidean distance) to the test observation. Then, the test observation is predicted to be in the same class as the majority of the closest training points. In the event of a tie, one of the classes is randomly chosen. It is necessary to normalize (generally min-max normalization instead of standardization) the data first since we will be calculating distances.</p>
<div id="knn-iris-example" class="section level2">
<h2><span class="header-section-number">3.1</span> KNN: Iris Example</h2>
<p>Normalize the iris data and split into a training and test set</p>
<pre class="r"><code>normalize &lt;- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
set.seed(123)
irisNormalized = as.data.frame(lapply(iris[,c(1,2,3,4)], normalize))
training_ind = sample(1:nrow(iris), 0.75 * nrow(iris))
train = irisNormalized[training_ind,]
test = irisNormalized[-training_ind,]
train_class = iris[training_ind,5]
test_class = iris[-training_ind,5]</code></pre>
<p>Now run knn function</p>
<pre class="r"><code>library(class)
test_prediction = knn(train, test, cl=train_class, k=3)</code></pre>
<p>Calculate accuracy of test data</p>
<pre class="r"><code>table(test_prediction, test_class)</code></pre>
<pre><code>##                test_class
## test_prediction setosa versicolor virginica
##      setosa         12          0         0
##      versicolor      0         16         1
##      virginica       0          1         8</code></pre>
<pre class="r"><code>mean(test_prediction == test_class)*100 #accuracy</code></pre>
<pre><code>## [1] 94.73684</code></pre>
<p>Try a larger value of k (consider more neighbors)</p>
<pre class="r"><code>test_prediction = knn(train, test, cl=train_class, k=11)
table(test_prediction, test_class)</code></pre>
<pre><code>##                test_class
## test_prediction setosa versicolor virginica
##      setosa         12          0         0
##      versicolor      0         16         0
##      virginica       0          1         9</code></pre>
<pre class="r"><code>mean(test_prediction == test_class)*100 #accuracy</code></pre>
<pre><code>## [1] 97.36842</code></pre>
<p>In this case, a larger k increased the accuracy of the model, however this was inconsistent across different splits of the training and test sets. We could use cross-validation to test different values of k (and different train/test sets) to determine the optimal number of neighbors to use.</p>
<p>In general, a smaller value of k means noise in the data will have a higher influence on the boundary causing the boundary to be more flexible. A larger value of k will have smoother decision boundaries which may not pick up on some boundaries of the data, and is more computationally expensive.</p>
<p>A general rule is to choose <span class="math inline">\(k=\sqrt{N}\)</span> where N is the number of samples in the training data.</p>
</div>
</div>
<div id="naive-bayes" class="section level1">
<h1><span class="header-section-number">4</span> Naive Bayes</h1>
<p>Bayes theorem is given as: <span class="math display">\[p(C_k|\textbf{x}) = \frac{p(C_k)p(\textbf{x}|C_k)}{p(\textbf{x})}\]</span> where  represents the features in the data, <span class="math inline">\(C_k\)</span> is a possible class, and <span class="math inline">\(p(C_k|\textbf{x})\)</span> is the probability of belonging to a class given the independent variables defined by . This is sometimes rewritten as <span class="math display">\[\text{posterior} = \frac{\text{prior}\times \text{likelihood}}{\text{evidence}}\]</span></p>
<p>The denominator of the fraction is constant, and the numeriator is equivalent to the joint probability model <span class="math inline">\(p(C_k,x_1,...,x_n)\)</span>.</p>
<p>If we assume all features in  are mutually independent (this assumption is what makes it <em>naive</em>), we can rewrite Bayes theorem as <span class="math display">\[p(C_k|x_1,...,x_n) \propto p(C_k) \prod_{i=1}^n p(x_i|C_k)\]</span></p>
<div id="naive-bayes-classifier" class="section level2">
<h2><span class="header-section-number">4.1</span> Naive Bayes Classifier</h2>
<p>Assign a class label <span class="math inline">\(\hat{y} = C_k\)</span> for some k where <span class="math display">\[\hat{y} = \text{argmax}_{k \in \{1,...,K\}}p(C_k)\prod_{i=1}^n p(x_i|C_k)\]</span></p>
</div>
<div id="naive-bayes-titanic-example" class="section level2">
<h2><span class="header-section-number">4.2</span> Naive Bayes: Titanic Example</h2>
<p>Naive Bayes is good to use when all the predictors are categorical, but can also be used on continuous predictors. It also works well when there are more than 2 classes to predict, unlike other classification methods (e.g. logistic regression). This example was inspired by <a href="https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/">Naive Bayes Blog Post</a></p>
<p>We try to predict if a passenger survived on the Titanic given the predictors: Class (1st, 2nd, 3rd, crew), Sex (M,F), and Age (child, adult).</p>
<pre class="r"><code>data(&quot;Titanic&quot;)
Titanic = as.data.frame(Titanic)
#Data currently summary data, expand for analysis
repeating_sequence=rep.int(seq_len(nrow(Titanic)), Titanic$Freq)
Titanic_df=Titanic[repeating_sequence,]
Titanic_df = Titanic_df %&gt;% select(-Freq)</code></pre>
<p>Fit Naive Bayes model</p>
<pre class="r"><code>library(e1071)
mod.NB = naiveBayes(Survived~., data=Titanic_df)
mod.NB</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##       No      Yes 
## 0.676965 0.323035 
## 
## Conditional probabilities:
##      Class
## Y            1st        2nd        3rd       Crew
##   No  0.08187919 0.11208054 0.35436242 0.45167785
##   Yes 0.28551336 0.16596343 0.25035162 0.29817159
## 
##      Sex
## Y           Male     Female
##   No  0.91543624 0.08456376
##   Yes 0.51617440 0.48382560
## 
##      Age
## Y          Child      Adult
##   No  0.03489933 0.96510067
##   Yes 0.08016878 0.91983122</code></pre>
<p>Make predictions and check accuracy</p>
<pre class="r"><code>Titanic_df$PredictedSurvived = predict(mod.NB, Titanic_df)
table(Titanic_df$PredictedSurvived, Titanic_df$Survived)</code></pre>
<pre><code>##      
##         No  Yes
##   No  1364  362
##   Yes  126  349</code></pre>
<pre class="r"><code>mean(Titanic_df$PredictedSurvived==Titanic_df$Survived)</code></pre>
<pre><code>## [1] 0.7782826</code></pre>
<p>The model predicts if a passenger survives with about 77.8% accuracy.</p>
</div>
<div id="laplace-correction" class="section level2">
<h2><span class="header-section-number">4.3</span> Laplace Correction</h2>
<p>Suppose there was a set of predictors with a 0 probability of one of the outcomes. For example, suppose none of the crew survived. Without any correction, our model will always predict a crew member will not survive, leaving no chance for an unforeseen circumstance.</p>
<pre class="r"><code>missingData = unique(Titanic_df %&gt;% filter(Class==&#39;Crew&#39;, Survived==&#39;Yes&#39;) %&gt;% select(Class:Survived))
Titanic_df_Less = Titanic_df %&gt;% select(Class:Survived) %&gt;% anti_join(missingData, by = c(&#39;Class&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Survived&#39;))
mod.NBNoLaplace = naiveBayes(Survived ~ ., data=Titanic_df_Less, laplace=0)
print(missingData[1,])</code></pre>
<pre><code>##   Class  Sex   Age Survived
## 1  Crew Male Adult      Yes</code></pre>
<pre class="r"><code>predict(mod.NBNoLaplace, missingData[1,], type=&quot;raw&quot;, threshold=0)</code></pre>
<pre><code>##      No Yes
## [1,]  1 NaN</code></pre>
<p>We introduce the Laplace corection, which adds 1 (or whatever defined value) to all the classes. This forces all joint probabilities to be greater than 0 and allows a (sometimes small) possibility for all outcomes.</p>
<pre class="r"><code>mod.NBLaplace = naiveBayes(Survived ~ ., data=Titanic_df_Less, laplace=1)
predict(mod.NBLaplace, missingData[1,], type=&quot;raw&quot;, threshold=0)</code></pre>
<pre><code>##             No         Yes
## [1,] 0.9994807 0.000519274</code></pre>
</div>
</div>
<div id="classification-trees" class="section level1">
<h1><span class="header-section-number">5</span> Classification Trees</h1>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
