<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Regression.html">Regression</a>
</li>
<li>
  <a href="Classification.html">Classification</a>
</li>
<li>
  <a href="Clustering.html">Clustering</a>
</li>
<li>
  <a href="Databases.html">Databases</a>
</li>
<li>
  <a href="CloudComputing.html">Cloud Computing</a>
</li>
<li>
  <a href="ShellGit.html">Shell/Git</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Regression</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#overview"><span class="toc-section-number">1</span> Overview</a><ul>
<li><a href="#correlation"><span class="toc-section-number">1.1</span> Correlation</a></li>
</ul></li>
<li><a href="#simple-linear-regression"><span class="toc-section-number">2</span> Simple Linear Regression</a><ul>
<li><a href="#assumptions"><span class="toc-section-number">2.1</span> Assumptions</a></li>
<li><a href="#fitting-the-model"><span class="toc-section-number">2.2</span> Fitting the Model</a></li>
<li><a href="#simple-linear-regression-cars-example"><span class="toc-section-number">2.3</span> Simple Linear Regression: Cars Example</a></li>
</ul></li>
<li><a href="#multiple-linear-regression"><span class="toc-section-number">3</span> Multiple Linear Regression</a><ul>
<li><a href="#multiple-linear-regression-credit-example"><span class="toc-section-number">3.1</span> Multiple Linear Regression: Credit Example</a></li>
<li><a href="#adding-an-interaction-cars-example"><span class="toc-section-number">3.2</span> Adding an Interaction: Cars Example</a></li>
<li><a href="#some-other-regression-topics"><span class="toc-section-number">3.3</span> Some Other Regression Topics</a></li>
</ul></li>
<li><a href="#comparing-means"><span class="toc-section-number">4</span> Comparing Means</a><ul>
<li><a href="#one-factor-anova-plant-growth-example"><span class="toc-section-number">4.1</span> One-Factor ANOVA: Plant Growth Example</a></li>
<li><a href="#multiple-comparisons"><span class="toc-section-number">4.2</span> Multiple Comparisons</a></li>
<li><a href="#two-factor-anova-strength-example"><span class="toc-section-number">4.3</span> Two-Factor ANOVA: Strength Example</a></li>
</ul></li>
</ul>
</div>

<div id="overview" class="section level1">
<h1><span class="header-section-number">1</span> Overview</h1>
<p>Regression typically refers to identifying a (linear) relationship between a set of explanatory variables and response variables. If the response variable is categorical, this typically refers to an ANOVA model.</p>
<div id="correlation" class="section level2">
<h2><span class="header-section-number">1.1</span> Correlation</h2>
<p>Correlation describes the strength and direction of a straight-line (linear) relationship between pair of variables. The Pearson correlation coefficient is defined as <span class="math display">\[r = \frac{cov(X,Y)}{s_X s_Y}\]</span> where <span class="math inline">\(s_X\)</span> is the standard deviation of X, or equivalently, <span class="math display">\[r = \frac{1}{n-1} \sum z_xz_y\]</span> where <span class="math inline">\(z_x = \frac{x-\bar{x}}{s_x}\)</span>. We can think of each <span class="math inline">\(z_x\)</span> as the number of standard deviations the data is above the mean.</p>
<p>Correlation ranges from -1 to 1, values with a larger magnitude indicate a stronger correlation, and the sign designates the direction. If the correlation is 0 we can conclude the variables are not linearly dependent. However, even if the correlation is small (in magnitude), there may exist a non-linear relationship between them. Correlation is unit free, unit invariant, and sensitive to outliers.</p>
</div>
</div>
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">2</span> Simple Linear Regression</h1>
<p>Simple linear regression occurs when there is one independent variable and one dependent variable (typically both continuous).</p>
<div id="assumptions" class="section level2">
<h2><span class="header-section-number">2.1</span> Assumptions</h2>
<p>Simple linear regression assumes a model of the form: <span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\]</span></p>
<p>We also make various assumptions when fitting a linear regression model:</p>
<ol style="list-style-type: decimal">
<li>All errors (<span class="math inline">\(\varepsilon_i\)</span>’s) are independent</li>
<li>Mean of <span class="math inline">\(\varepsilon\)</span> at any fixed x is 0, so average of all <span class="math inline">\(\varepsilon\)</span> is 0</li>
<li>At any value of x, the spread of the y’s is the same as any other value of x <span class="math inline">\(\rightarrow\)</span> Homoscedasticity
<ul>
<li><span class="math inline">\(Var(\varepsilon_{ij}) = \sigma^2 = MSE\)</span></li>
</ul></li>
<li>At any fixed x, the distribution of <span class="math inline">\(\varepsilon\)</span> is normal</li>
</ol>
<p>We generally assume assumptions 1 and 2 are true, and have methods of verifying assumptions 3 and 4 (explored with examples).</p>
</div>
<div id="fitting-the-model" class="section level2">
<h2><span class="header-section-number">2.2</span> Fitting the Model</h2>
<p>To fit a linear regression model, we want to minimize the sum of squares residuals or sum of squared estimate of errors: <span class="math inline">\(SSE = \sum_{i=1}^n e_i^2 = \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2\)</span> where <span class="math inline">\(\hat{y}_i\)</span> are the fitted values. Using calculus, it can be shown the solution to this criteria is:</p>
<p><span class="math display">\[\hat{\beta_1} = \frac{\sum\limits_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum\limits_{i=1}^n(X_i-\bar{X})^2}, \beta_0 = \bar{Y}-\hat{\beta}_1\bar{X}\]</span></p>
<p>Alternatively, we can use a linear algebra approach and look for a least squares solution to <span class="math inline">\(A\bar{x} = \bar{b}\)</span> where <span class="math display">\[A = \begin{bmatrix}
1 &amp; x_1\\
1 &amp; x_2\\
\vdots &amp; \vdots\\
1 &amp; x_n
\end{bmatrix}, x = \begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}, b = \begin{bmatrix}
y_1\\
y_2\\
 \vdots\\
y_n
\end{bmatrix}\]</span>.</p>
<p>However, <span class="math inline">\(\bar{b}\)</span> is not in the column space of <span class="math inline">\(A\)</span>, so we must search for a solution for <span class="math inline">\(A\bar{x} = \bar{b}_{||} = \bar{b}-\bar{b}_\perp\)</span>. Multiplying by the transpose of <span class="math inline">\(A\)</span> we have: <span class="math inline">\(A^TA\bar{x} = A^T\bar{b} - A^T\bar{b}_\perp = A^T\bar{b} + 0\)</span>. Finally, solving for <span class="math inline">\(\bar{x}\)</span> we have: <span class="math inline">\(\bar{x} = (A^TA)^{-1}A^T\bar{b}\)</span>.</p>
</div>
<div id="simple-linear-regression-cars-example" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">2.3</span> Simple Linear Regression: Cars Example</h2>
<p>We will use the <em>cars</em> data to predict the stopping distance of a car given it’s speed. Begin by plotting the data and calculating the correlation:</p>
<pre class="r"><code>library(ggplot2)
ggplot(cars, aes(speed, dist)) + geom_point()</code></pre>
<p><img src="Regression_files/figure-html/carplot-1.png" width="672" /></p>
<pre class="r"><code>cor(cars)</code></pre>
<pre><code>##           speed      dist
## speed 1.0000000 0.8068949
## dist  0.8068949 1.0000000</code></pre>
<p>The relationship appears linear and the correlation is sufficiently large (0.807), so we proceed with fitting a model.</p>
<pre class="r"><code>mod.LinearCar = lm(dist~speed, data=cars)
summary(mod.LinearCar)</code></pre>
<pre><code>## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.069  -9.525  -2.272   9.215  43.201 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -17.5791     6.7584  -2.601   0.0123 *  
## speed         3.9324     0.4155   9.464 1.49e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.38 on 48 degrees of freedom
## Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 
## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</code></pre>
<p>The coefficients, as well as their standard error and <code>importance</code> measure are given. We are also given the <em>Residual standard error</em> = <span class="math inline">\(\hat{\sigma}\)</span> and <em>Multiple R-squared</em> which in this case (since it’s a simple linear model) is equal to the square of the correlation. The p-value associated with the variables test if we can drop the variable from the model, a large p-value indicates we can drop.</p>
<p>Additionally, we can view the ANOVA table of the model.</p>
<pre class="r"><code>anova(mod.LinearCar)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: dist
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## speed      1  21186 21185.5  89.567 1.49e-12 ***
## Residuals 48  11354   236.5                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see the model sum of squares (SSM) = <span class="math inline">\(\sum\limits_{i=1}^n(\hat{y}_i-\bar{y})^2\)</span> (in this case SSM = 21186). We are also shown SSE = <span class="math inline">\(\sum\limits_{i=1}^n(y_i-\hat{y})^2\)</span> (in this case SSE = 11354). We could calculate the sum of squares total as SST = SSM + SSE.</p>
<div id="making-predictions" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Making Predictions</h3>
<p>If we wanted to predicted the <em>average</em> stopping distance for a car with speed = 15, we can construct a <em>confidence</em> interval with:</p>
<pre class="r"><code>predict(mod.LinearCar, data.frame(speed=15), se.fit=T, interval=&#39;confidence&#39;,level=0.95)$fit</code></pre>
<pre><code>##        fit      lwr      upr
## 1 41.40704 37.02115 45.79292</code></pre>
<p>However, we could also predict the stopping distance for a <em>particular</em> car with speed = 15 by constructing a <em>prediction</em> interval:</p>
<pre class="r"><code>predict(mod.LinearCar, data.frame(speed=15), se.fit=T, interval=&#39;prediction&#39;,level=0.95)$fit</code></pre>
<pre><code>##        fit      lwr      upr
## 1 41.40704 10.17482 72.63925</code></pre>
<p>The estimates in both cases are the same, however the <em>confidence</em> interval only considers variation from repeated experiments while a <em>prediction</em> interval considers this variance <strong>and</strong> the variation of an individual. For either case, the prediction will be more reliable near the center of the data.</p>
<p><strong>Adding Line to Plot</strong></p>
<p>We can easily add a linear model to our plot in the ggplot2 package. This also defaults to drawing a 95% confidence interval.</p>
<pre class="r"><code>ggplot(cars, aes(speed, dist)) + geom_point() + geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="Regression_files/figure-html/carplot2-1.png" width="672" /></p>
</div>
<div id="checking-model-assumptions" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Checking Model Assumptions</h3>
<p><strong>Homoscedasticity</strong></p>
<p>We assumed the spread of y’s is the same at any value of x. To check this,</p>
<pre class="r"><code>plot(fitted.values(mod.LinearCar), residuals(mod.LinearCar), pch=16, xlab=&#39;Predicted Value&#39;, ylab=&#39;Residual&#39;)
abline(h=0, lty=2)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The residuals should be close to 0 and not have any pattern. If a pattern does exist (e.g. a funnel/triangle shape), we have evidence of non-homoscedasticity which could potentially be fixed by a data transformation.</p>
<p><strong>Normality of Residuals</strong></p>
<p>We also assumed our errors were normally distributed. To check this we will construct a QQ-plot:</p>
<pre class="r"><code>qqnorm(residuals(mod.LinearCar), pch=16)
qqline(residuals(mod.LinearCar), col = &quot;red&quot;, lwd = 2)</code></pre>
<p><img src="Regression_files/figure-html/carqq-1.png" width="672" /></p>
<p>In this plot, <em>Sample Quantiles</em> refer to the actual value of the residuals, while <em>Theoretical Quantiles</em> are the z-scores of the residuals. If the residuals are normally distributed, the points should fall on/close to the reference line.</p>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level1">
<h1><span class="header-section-number">3</span> Multiple Linear Regression</h1>
<p>Multiple linear regression is an extension of simple linear regression when there are several independent variables or functions of independent variables. For example, we could have two independent variables (<span class="math inline">\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i\)</span>), a quadratic term (<span class="math inline">\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i1}^2 + \varepsilon_i\)</span>), or including an interaction term (<span class="math inline">\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \varepsilon_i\)</span>). There can be non-linearity in x as long as the coefficients (<span class="math inline">\(\beta_i\)</span>) maintain a linear relationship.</p>
<div id="multiple-linear-regression-credit-example" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">3.1</span> Multiple Linear Regression: Credit Example</h2>
<p>We explore various models to predict credit card balance. This example was inspired by Springer’s book: <em>An Introduction to Statistical Learning</em></p>
<p>We start with a model using all the available variables (except for ID). Note categorical variables are formatted as factors and are automatically converted to indicator/dummy variables.</p>
<pre class="r"><code>library(ISLR)
head(Credit)</code></pre>
<pre><code>##   ID  Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance
## 1  1  14.891  3606    283     2  34        11   Male      No     Yes Caucasian     333
## 2  2 106.025  6645    483     3  82        15 Female     Yes     Yes     Asian     903
## 3  3 104.593  7075    514     4  71        11   Male      No      No     Asian     580
## 4  4 148.924  9504    681     3  36        11 Female      No      No     Asian     964
## 5  5  55.882  4897    357     2  68        16   Male      No     Yes Caucasian     331
## 6  6  80.180  8047    569     4  77        10   Male      No      No Caucasian    1151</code></pre>
<pre class="r"><code>mod.Full = lm(Balance ~ Cards + Limit + Rating + Age + Gender + Student + Income + Education + Married + Ethnicity, data=Credit)
summary(mod.Full)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Balance ~ Cards + Limit + Rating + Age + Gender + 
##     Student + Income + Education + Married + Ethnicity, data = Credit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -161.64  -77.70  -13.49   53.98  318.20 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        -479.20787   35.77394 -13.395  &lt; 2e-16 ***
## Cards                17.72448    4.34103   4.083 5.40e-05 ***
## Limit                 0.19091    0.03278   5.824 1.21e-08 ***
## Rating                1.13653    0.49089   2.315   0.0211 *  
## Age                  -0.61391    0.29399  -2.088   0.0374 *  
## GenderFemale        -10.65325    9.91400  -1.075   0.2832    
## StudentYes          425.74736   16.72258  25.459  &lt; 2e-16 ***
## Income               -7.80310    0.23423 -33.314  &lt; 2e-16 ***
## Education            -1.09886    1.59795  -0.688   0.4921    
## MarriedYes           -8.53390   10.36287  -0.824   0.4107    
## EthnicityAsian       16.80418   14.11906   1.190   0.2347    
## EthnicityCaucasian   10.10703   12.20992   0.828   0.4083    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 98.79 on 388 degrees of freedom
## Multiple R-squared:  0.9551, Adjusted R-squared:  0.9538 
## F-statistic: 750.3 on 11 and 388 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>From our output, we see <em>Gender, Education, Married, Ethnicity</em> are not significant, so we remove them from the model:</p>
<pre class="r"><code>mod.Less = lm(Balance ~ Cards + Limit + Rating + Age + Student + Income, data=Credit)
summary(mod.Less)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Balance ~ Cards + Limit + Rating + Age + Student + 
##     Income, data = Credit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -170.00  -77.85  -11.84   56.87  313.52 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -493.73419   24.82476 -19.889  &lt; 2e-16 ***
## Cards         18.21190    4.31865   4.217 3.08e-05 ***
## Limit          0.19369    0.03238   5.981 4.98e-09 ***
## Rating         1.09119    0.48480   2.251   0.0250 *  
## Age           -0.62406    0.29182  -2.139   0.0331 *  
## StudentYes   425.60994   16.50956  25.780  &lt; 2e-16 ***
## Income        -7.79508    0.23342 -33.395  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 98.61 on 393 degrees of freedom
## Multiple R-squared:  0.9547, Adjusted R-squared:  0.954 
## F-statistic:  1380 on 6 and 393 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Removing these variables results in a simplier model without sacrificing performance (<span class="math inline">\(R^2\)</span> is similar).</p>
<div id="consider-multicollinearity" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Consider Multicollinearity</h3>
<p>It is important our variables are not highly correlated with each other so that we have a design matrix of full rank and a unique solution to our minimization problem. To check this, we can compute the variance inflation factor for our variables, defined as <span class="math display">\[VIF(X_m) = \frac{1}{1-R^2_m}\]</span> where <span class="math inline">\(R^2_m\)</span> is the coefficient of determination when <span class="math inline">\(X_m\)</span> is regressed on all of the other predictors. We want <span class="math inline">\(VIF(X_m)\)</span> to be close to 1, and a general rule is if it is larger than 5 or 10 there is a problem of multicollinearity.</p>
<p>For our simpler model, we calculate the VIF for each variable:</p>
<pre class="r"><code>library(car)
vif(mod.Less)</code></pre>
<pre><code>##      Cards      Limit     Rating        Age    Student     Income 
##   1.439007 229.238479 230.869514   1.039696   1.009064   2.776906</code></pre>
<p>The VIF for <em>Limit</em> and <em>Rating</em> is extremely high, indicating we should not include both of the variables in the model. We can calculate the correlation of these two variables and see it is close to 1.</p>
<pre class="r"><code>cor(Credit$Limit, Credit$Rating)</code></pre>
<pre><code>## [1] 0.9968797</code></pre>
<pre class="r"><code>mod.LessNoRating = lm(Balance ~ Cards + Limit + Age + Student + Income, data=Credit)
summary(mod.LessNoRating)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Balance ~ Cards + Limit + Age + Student + Income, 
##     data = Credit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -187.05  -79.57  -12.59   56.06  322.56 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -4.673e+02  2.199e+01 -21.250  &lt; 2e-16 ***
## Cards        2.355e+01  3.628e+00   6.492 2.55e-10 ***
## Limit        2.661e-01  3.535e-03  75.296  &lt; 2e-16 ***
## Age         -6.220e-01  2.933e-01  -2.120   0.0346 *  
## StudentYes   4.284e+02  1.655e+01  25.886  &lt; 2e-16 ***
## Income      -7.760e+00  2.341e-01 -33.149  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 99.12 on 394 degrees of freedom
## Multiple R-squared:  0.9541, Adjusted R-squared:  0.9535 
## F-statistic:  1638 on 5 and 394 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="checking-model-assumptions-1" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Checking Model Assumptions</h3>
<p>Just like with the simple linear example, we need to verify our assumptions. We can plot our model to look for evidence of heteroscedasticity and non-normality:</p>
<pre class="r"><code>plot(mod.LessNoRating, which=1)</code></pre>
<p><img src="Regression_files/figure-html/assumptions-1.png" width="672" /></p>
<pre class="r"><code>mod.LimitIncome = lm(Balance ~ Limit + Income, data=Credit)
plot(mod.LimitIncome, which=1)</code></pre>
<p><img src="Regression_files/figure-html/assumptions-2.png" width="672" /></p>
<p>There are some concerns with our residuals which seem to be caused when both <em>Limit</em> and <em>Income</em> are included. The U-shape of the residuals indicate there may be something wrong with our model structure, and perhaps a quadratic term should be added. However, this U-shape only exists when both <em>Limit</em> and <em>Income</em> are included. If we remove one of these (removing <em>Income</em> resulted in a smaller drop in <span class="math inline">\(R^2\)</span>), the concerns of heteroscedasticity are reduced.</p>
<pre class="r"><code>mod.LessNoIncome = lm(Balance ~ Cards + Limit + Age + Student, data=Credit)
summary(mod.LessNoIncome)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Balance ~ Cards + Limit + Age + Student, data = Credit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -657.27 -118.27   -2.56  137.66  449.07 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.074e+02  4.171e+01  -7.369 1.01e-12 ***
## Cards        2.950e+01  7.044e+00   4.188 3.48e-05 ***
## Limit        1.734e-01  4.201e-03  41.282  &lt; 2e-16 ***
## Age         -2.183e+00  5.628e-01  -3.878 0.000123 ***
## StudentYes   4.043e+02  3.214e+01  12.578  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 192.7 on 395 degrees of freedom
## Multiple R-squared:  0.8261, Adjusted R-squared:  0.8243 
## F-statistic: 469.1 on 4 and 395 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(mod.LessNoIncome, which=1)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We can also check our residuals are normally distributed:</p>
<pre class="r"><code>plot(mod.LessNoIncome, which=2)</code></pre>
<p><img src="Regression_files/figure-html/normality-1.png" width="672" /></p>
</div>
</div>
<div id="adding-an-interaction-cars-example" class="section level2">
<h2><span class="header-section-number">3.2</span> Adding an Interaction: Cars Example</h2>
<p>So far we have assumed the effect of one predictor variable is independent of another, so all of our predictor variables are additive. However, it’s possible the variables are not independent and there is instead an interaction effect.</p>
<p>Using the <em>mtcars</em> data set, we try to predict miles per gallon (<em>mpg</em>) using horsepower (<em>hp</em>) and engine type (<em>vs</em>: v-shaped/straight). Without an interaction effect, our model produces two parallel lines, depending on the value of <em>vs</em>.</p>
<pre class="r"><code>mod.NoInt = lm(mpg~hp+vs, mtcars)
mtcarsNoInt = cbind(mtcars, mpg.Fit = predict(mod.NoInt))
ggplot(mtcarsNoInt, aes(x = hp, y = mpg, colour = factor(vs))) + geom_point() + geom_line(aes(y=mpg.Fit))</code></pre>
<p><img src="Regression_files/figure-html/nointeraction-1.png" width="672" /></p>
<p>However, it is likely <em>hp</em> has a different effect on <em>mpg</em> depending on the engine type (<em>vp</em>). We add the interaction term to the model and see the lines are no longer parallel.</p>
<pre class="r"><code>mod.Int = lm(mpg~hp+vs+hp*vs, mtcars)
mtcarsInt = cbind(mtcars, mpg.Fit = predict(mod.Int))
ggplot(mtcarsInt, aes(x = hp, y = mpg, colour = factor(vs))) + geom_point() + geom_line(aes(y=mpg.Fit))</code></pre>
<p><img src="Regression_files/figure-html/interaction-1.png" width="672" /></p>
<p>We can also examine the coefficients of our models:</p>
<pre class="r"><code>summary(mod.NoInt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ hp + vs, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7131 -2.3336 -0.1332  1.9055  7.9055 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 26.96300    2.89069   9.328 3.13e-10 ***
## hp          -0.05453    0.01448  -3.766 0.000752 ***
## vs           2.57622    1.96966   1.308 0.201163    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.818 on 29 degrees of freedom
## Multiple R-squared:  0.6246, Adjusted R-squared:  0.5987 
## F-statistic: 24.12 on 2 and 29 DF,  p-value: 6.768e-07</code></pre>
<pre class="r"><code>summary(mod.Int)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ hp + vs + hp * vs, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.5821 -1.7710 -0.3612  1.5969  9.2646 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 24.49637    2.73893   8.944 1.07e-09 ***
## hp          -0.04153    0.01379  -3.011  0.00547 ** 
## vs          14.50418    4.58160   3.166  0.00371 ** 
## hp:vs       -0.11657    0.04130  -2.822  0.00868 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.428 on 28 degrees of freedom
## Multiple R-squared:  0.7077, Adjusted R-squared:  0.6764 
## F-statistic:  22.6 on 3 and 28 DF,  p-value: 1.227e-07</code></pre>
<p>The model with interaction has a larger <span class="math inline">\(R^2\)</span> than the model without. It is interested that <em>vs</em> is not significant in the first model but becomes significant when the interaction term is added.</p>
<p>The hierarchical principle tells us that within a model if the main effects variable (e.g. <em>vs</em>) is not significant but the interaction variable is (e.g. <em>hp:vs</em>), then we should still include the main effects variable. In our example, all the main effects variables were shown to be significant, but if one hadn’t and the interaction was significant, we should still include them.</p>
<p>A <em>non-rigorous</em> way to tell if an interaction variable is needed is to plot the data and visually fit a line by group. If the lines are parallel, no interaction is needed, but if they intersect (in this case), then an interaction should be considered.</p>
</div>
<div id="some-other-regression-topics" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">3.3</span> Some Other Regression Topics</h2>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Model Selection</h3>
<p>We have been using <span class="math inline">\(R^2\)</span> to determine the percent of variation in the data is described by the model. We are also given the residual standard error, or root mean squared error (RMSE), in the model summary output.</p>
<p><strong>Other model metrics</strong></p>
<ul>
<li>Mallow’s <span class="math inline">\(C_p\)</span>: If this is small the model is competitive to the full model and preserves degrees of freedom</li>
<li>Adjusted <span class="math inline">\(R^2\)</span>: Penalizes for too many predictors that don’t reduce unexplained variation</li>
<li><span class="math inline">\(PRESS_p\)</span>: Prediction Sum of Squares, if small then predicts point well</li>
<li>BIC/AIC: Small if model is good fit and simple</li>
</ul>
<p><code>Best subsets</code> can be used to determine the best k models for a chosen number of variables.</p>
<p><code>Stepwise algorithms</code> add/remove independent variables one at a time before converging to a <em>best</em> model. They can be either forward selection, backward elimination, or both. The algorithm may converge to different <em>best</em> models depending on the direction and initial model.</p>
</div>
<div id="leverage-and-influence" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Leverage and Influence</h3>
<ul>
<li>Leverage: Distance of an observation from the mean of the explanatory variables. The inclusion/exclusion of this observation has a large change on the fitted line
<ul>
<li>Hat values - indicate potential for leverage</li>
<li>Press residuals</li>
<li>Studentized residuals</li>
</ul></li>
</ul>
<pre class="r"><code>plot(mod.LessNoIncome, which=3)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<ul>
<li>Influence: Ability to change metrics of lines fit. For example, <span class="math inline">\(R^2 = 1-\frac{SSE}{SST}\)</span>, so adding a data point increases SST while it may keep SSE the same, artificially inflating <span class="math inline">\(R^2\)</span>.
<ul>
<li>DFFITS - Difference in fits</li>
<li>DfBetas - How much coefficients change when ith value is deleted</li>
<li>Cook’s Distance - measures overall influence</li>
</ul></li>
</ul>
<pre class="r"><code>plot(mod.LessNoIncome, which=4)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="comparing-means" class="section level1">
<h1><span class="header-section-number">4</span> Comparing Means</h1>
<p>One-way analysis of variance (one-way ANOVA) allows us to compare means between two or more samples, where the repose variable is numeric and there is one explanatory variable (hence, one-way) that is generally categorical.</p>
<p>The one-factor ANOVA model is given by either the <em>means model</em> <span class="math display">\[Y_{i,j} = \mu_i + \varepsilon_{i,j}\]</span> or by the <em>main effects model</em> <span class="math display">\[Y_{i,j} = \mu + \alpha_i + \varepsilon_{i,j}\]</span> where <span class="math inline">\(Y_{i,j}\)</span> is the jth observation at the ith treatment level, <span class="math inline">\(\mu\)</span> is the grand mean, <span class="math inline">\(\mu_i\)</span> is the mean of the observation for the ith treatment group, and <span class="math inline">\(\alpha_i\)</span> is the ith treatment effect (deviation from the grand mean).</p>
<p>We have a balanced design if <span class="math inline">\(n_i\)</span> is the same for all groups. If we think some treatment group has more variation we could use an unbalanced design.</p>
<p>We could also consider the two-factor ANOVA model which considers the influence of two explanatory variables. The (full) model is given by <span class="math display">\[Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \varepsilon_{ijk}\]</span> where <span class="math inline">\(Y_{ijk}\)</span> is the kth observation at the ith and jth treatment level, and <span class="math inline">\((\alpha\beta)_{ij}\)</span> is the interaction effect.</p>
<p>ANOVA models perform an F-test to determine if all the group means are the same, or if at least one is statistically different. Various comparison and confidence interval methods allow us to determine which group is different.</p>
<p>The ANOVA model is a generalization of the simple linear regression model so it follows the same assumptions.</p>
<div id="one-factor-anova-plant-growth-example" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">4.1</span> One-Factor ANOVA: Plant Growth Example</h2>
<p>This example was inspired by <a href="http://www.sthda.com/english/wiki/one-way-anova-test-in-r#visualize-your-data-and-compute-one-way-anova-in-r">One-way ANOVA</a></p>
<div id="fitting-anova-model" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Fitting ANOVA Model</h3>
<p>Our dataset include the weights of plants in a control group and two treatment groups. Notice the <em>group</em> column is formatted as a factor.</p>
<pre class="r"><code>library(ggplot2)
growth = PlantGrowth
str(growth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  2 variables:
##  $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...
##  $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>ggplot(growth, aes(group, weight)) + geom_boxplot()</code></pre>
<p><img src="Regression_files/figure-html/oneAnova-1.png" width="672" /></p>
<p>The boxplot indicates there may be a difference between treatments 1 and 2. We compute the one-way ANOVA test.</p>
<pre class="r"><code>mod.aov = aov(weight~group, data = growth)
summary(mod.aov)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## group        2  3.766  1.8832   4.846 0.0159 *
## Residuals   27 10.492  0.3886                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The p-value is less than 0.05 (our significance level), so there is evidence of a difference between some of the group means.</p>
</div>
<div id="checking-assumptions" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Checking Assumptions</h3>
<p><strong>Homoscedasticity</strong></p>
<p>Like with the regression examples, we look for homogeneity of variances with a plot.</p>
<pre class="r"><code>plot(mod.aov, which=1)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We could also use the <em>Levene’s Test</em>. For this, we fit an ANOVA model of the absolute residuals from our model against the groups. The F-statistic tests if there is a difference in the absolute mean residuals between the groups. If the test does detect a difference (small p-value), then there may be evidence of heteroscedasticity.</p>
<p>There is also a function in the <em>car</em> package to run the Levene’s Test.</p>
<pre class="r"><code>summary(aov(abs(mod.aov$residuals)~group, data=growth))</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)
## group        2  0.332  0.1662   1.237  0.306
## Residuals   27  3.628  0.1344</code></pre>
<pre class="r"><code>library(car)
leveneTest(weight~group, data=growth)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  2  1.1192 0.3412
##       27</code></pre>
<p><strong>Normality of Residuals</strong></p>
<p>Again, we can check the normality of the residuals with a plot.</p>
<pre class="r"><code>plot(mod.aov, which=2)</code></pre>
<p><img src="Regression_files/figure-html/normalityAOV-1.png" width="672" /></p>
<p>Note: observations 4, 15, and 17 may be outliers.</p>
<p>We can also use the <em>Shapiro-Wilk</em> test to determine if the residuals came from a normally distributed population.</p>
<pre class="r"><code>shapiro.test(mod.aov$residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  mod.aov$residuals
## W = 0.96607, p-value = 0.4379</code></pre>
<p>A large p-value means that we don’t reject the null hypothesis that our residuals are normally distributed.</p>
</div>
<div id="nonparametric-alternative" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Nonparametric Alternative</h3>
<p>If the assumptions of the ANOVA are not met, we could use the <em>Kruskal-Wallis</em> test that does not assume a normal distribution of residuals. This test ranks all the data (ignoring group membership), computes a test statistic, then tests if all the groups have the same distribution.</p>
<p>The Dunn test can then be used to compute all pairwise comparisons (similar to Tukey’s pairwise comparisons method for ANOVA).</p>
<pre class="r"><code>kruskal.test(weight~group, data=growth)</code></pre>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  weight by group
## Kruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842</code></pre>
<pre class="r"><code>library(FSA)
DT = dunnTest(weight~group, data=growth, method=&#39;bh&#39;)
DT</code></pre>
<pre><code>##    Comparison         Z    P.unadj      P.adj
## 1 ctrl - trt1  1.117725 0.26368427 0.26368427
## 2 ctrl - trt2 -1.689290 0.09116394 0.13674592
## 3 trt1 - trt2 -2.807015 0.00500029 0.01500087</code></pre>
<pre class="r"><code>library(rcompanion)
cldList(P.adj~Comparison, data = DT$res, threshold=0.05)</code></pre>
<pre><code>##   Group Letter MonoLetter
## 1  ctrl     ab         ab
## 2  trt1      a         a 
## 3  trt2      b          b</code></pre>
<p>The large p-value from the Kruskal-Wallis test tells us our groups come from different distributions. The Dunn test tells us there is a difference between treatment 1 and treatment 2.</p>
</div>
</div>
<div id="multiple-comparisons" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">4.2</span> Multiple Comparisons</h2>
<p>The ANOVA test will determine if there is a difference between some of the groups, but won’t immediately tell us what groups are different. We can compute <em>pairwise confidence intervals</em> to determine which groups are different.</p>
<p>The general structure of a confidence interval is <span class="math inline">\(estimate \pm t_{dfe}*SE\)</span>, and various <span class="math inline">\(t_{dfe}\)</span> and standard errors are possible.</p>
<div id="t-interval" class="section level3">
<h3><span class="header-section-number">4.2.1</span> T-Interval</h3>
<p><strong>Welch’s t-interval</strong> (Unpooled)</p>
<p>Welch’s t-interval is useful if we don’t think the population variances of the groups are equal. The standard error is given by <span class="math inline">\(\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}\)</span> where the standard deviations of the groups are kept separated.</p>
<p>In R, we calculate this by</p>
<pre class="r"><code>control = growth$weight[growth$group==&#39;ctrl&#39;] 
treat1 = growth$weight[growth$group==&#39;trt1&#39;]
treat2 = growth$weight[growth$group==&#39;trt2&#39;] 
t.test(control, treat1, var.equal=F, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -0.2875162  1.0295162
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>t.test(control, treat2, var.equal=F, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -0.98287213 -0.00512787
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>t.test(treat1, treat2, var.equal=F, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -1.4809144 -0.2490856
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<p>According to these intervals, there is a difference between control and treatment 2 and between treatment 1 and treatment 2.</p>
<p><strong>Pooled t-interval</strong></p>
<p>When we created the ANOVA model, we showed there was not a significant difference in variance between the groups, so we are able to pool the variance. In this case, our standard error is given by <span class="math inline">\(s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span> where <span class="math inline">\(s_p\)</span> is the pooled variance equal to <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<p>In R, the confidence intervals are calculated similarly to the unpooled intervals, except now the variance is equal.</p>
<pre class="r"><code>t.test(control, treat1, var.equal=T, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -0.2833003  1.0253003
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>t.test(control, treat2, var.equal=T, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -0.980338117 -0.007661883
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>t.test(treat1, treat2, var.equal=T, alternative = &#39;two.sided&#39;)$conf.int</code></pre>
<pre><code>## [1] -1.4687336 -0.2612664
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>pairwise.t.test(growth$weight, growth$group)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  growth$weight and growth$group 
## 
##      ctrl  trt1 
## trt1 0.194 -    
## trt2 0.175 0.013
## 
## P value adjustment method: holm</code></pre>
<p>The intervals are slightly different than the unpooled intervals.</p>
<p>Using the standard t confidence interval to construct all the pair-wise confidence intervals is generally not recommended because it doesn’t consider the family-wise error rate, and results in artificially small intervals. This method should only be used if we’re interested in one pairwise contrast which was decided upon beofre looking at the data.</p>
</div>
<div id="family-wise-error-rate" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Family-wise error rate</h3>
<p>Suppose our significance level (<span class="math inline">\(\alpha\)</span>) is 0.05. If we construct one 95% confidence interval, the probability of making a type I error is 0.05.</p>
<p>The family-wise error rate is the probability of making at least one type I error in the family, defined as <span class="math display">\[FWER \leq 1-(1-\alpha)^{I \choose 2} \]</span> where <span class="math inline">\(\alpha\)</span> is the significance level for an individual test and <span class="math inline">\({I \choose 2}\)</span> is the number of pairwise comparisons given the total number of groups I.</p>
<p>If there are 3 groups, then there are 3 pairwise comparisons, and <span class="math inline">\(FWER \leq 0.1426\)</span>. If there are 5 groups, then there are 10 pairwise comparisons, and <span class="math inline">\(FWER \leq 0.4013\)</span>, etc.</p>
<p>As the number of groups grows, the upper bound for FWER also grows. So, as increase groups, the probability of a type I error becomes very high. Some ways to control this include the Bonferroni correction, Tukey’s procedure, and Scheffe’s method</p>
</div>
<div id="bonferonni-correction" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Bonferonni Correction</h3>
<p>The Bonferroni correction constructs each individual confidence interval at a higher level to result in a higher family-wise confidence level. The individual confidence level is <span class="math inline">\(\frac{\alpha}{m}\)</span> where m is the total number of intervals to construct (often <span class="math inline">\(m = {I \choose 2}\)</span>). So, each confidence interval will have a <span class="math inline">\(1-\frac{\alpha}{m}\)</span> level of confidence. The Bonferonni correction is typically only recommended for 3 or 4 groups.</p>
<p>For our example, we had 3 groups so each pairwise confidence interval will be constructed at the 98.3% confidence level.</p>
<pre class="r"><code>library(DescTools)
PostHocTest(mod.aov, method=&#39;bonferroni&#39;)</code></pre>
<pre><code>## 
##   Posthoc multiple comparisons of means : Bonferroni 
##     95% family-wise confidence level
## 
## $group
##             diff     lwr.ci    upr.ci   pval    
## trt1-ctrl -0.371 -1.0825786 0.3405786 0.5832    
## trt2-ctrl  0.494 -0.2175786 1.2055786 0.2630    
## trt2-trt1  0.865  0.1534214 1.5765786 0.0134 *  
## 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>These intervals tell us only the difference between treatment 1 and treatment 2 are stastically significant.</p>
</div>
<div id="tukey-procedure" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Tukey Procedure</h3>
<p>The Tukey method uses the studentized range distribution to create simultaneous comparisons for all pairwise comparisons.</p>
<p>We can compute all the pairwise Tukey intervals in R.</p>
<pre class="r"><code>TukeyHSD(mod.aov)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = weight ~ group, data = growth)
## 
## $group
##             diff        lwr       upr     p adj
## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711
## trt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960
## trt2-trt1  0.865  0.1737839 1.5562161 0.0120064</code></pre>
<p>We can also visualize the intervals</p>
<pre class="r"><code>plot(TukeyHSD(mod.aov))</code></pre>
<p><img src="Regression_files/figure-html/tukeyplot-1.png" width="672" /></p>
<p>The conclusion is the same as with the Bonferonni correction (treatment 1 and 2 are different), however the Tukey method works well for many groups.</p>
</div>
<div id="scheffes-method" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Scheffe’s Method</h3>
<p>Scheffe’s method is useful for any type of contrast, not just pairwise differences. A confidence interval for contrast <span class="math inline">\(\Psi = \sum\limits_{i=1}^Ic_iy_i\)</span> is given by <span class="math display">\[\sum\limits_{i=1}^Ic_i\hat{y}_i \pm \sqrt{(I-1)F_{I-1,n-1}^\alpha}*\hat{\sigma}\sqrt{\sum\limits_{i=1}^I\frac{c_i^2}{n_i}}\]</span> For example, suppose we want a confidence interval for <span class="math inline">\(y_{control} - \frac{y_{treat1} + y_{treat2}}{2}\)</span> which compares control to the average of the treatments.</p>
<pre class="r"><code>ScheffeTest(mod.aov, contrasts = matrix(c(1,-0.5,-0.5,
                                          -0.5,1,-0.5,
                                          -0.5,-0.5,1),ncol=3))</code></pre>
<pre><code>## 
##   Posthoc multiple comparisons of means : Scheffe Test 
##     95% family-wise confidence level
## 
## $group
##                   diff     lwr.ci      upr.ci   pval    
## ctrl-trt1,trt2 -0.0615 -0.6868163 0.563816298 0.9681    
## trt1-ctrl,trt2 -0.6180 -1.2433163 0.007316298 0.0532 .  
## trt2-ctrl,trt1  0.6795  0.0541837 1.304816298 0.0310 *  
## 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The first row of the output table shows there is not a significant difference between the control group and the average of the treatment groups.</p>
<p>The second row shows there may be a difference between treatment 1 and the average of control and treatment 2.</p>
<p>The third row shows there is likely a difference between treatment 2 and the average between control and treatment 1.</p>
</div>
</div>
<div id="two-factor-anova-strength-example" class="section level2 tabset tabset-fade tabset-pills">
<h2><span class="header-section-number">4.3</span> Two-Factor ANOVA: Strength Example</h2>
<p>Data for this example were modified from an assignment in one of my graduate courses. Our goal is to determine if substrate or bonding material are useful in predicting strength.</p>
<div id="analyzing-the-data" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Analyzing the Data</h3>
<pre class="r"><code>strength = c(1.51,1.96,1.83,1.98, 2.62,2.82,2.69,2.93, 2.96,2.82,3.11,3.11, 3.67,3.4,3.25,2.9,
             1.63,1.8,1.92,1.71, 3.12,2.94,3.23,2.99, 2.91,2.93,3.01,2.93, 3.48,3.51,3.24,3.45,
             3.04,3.16,3.09,3.5, 1.19,2.11,1.78,2.25, 3.04,2.91,2.48,2.83, 3.47,3.42,3.31,3.76)
substrate = c(rep(&quot;A&quot;,16), rep(&quot;B&quot;, 16), rep(&quot;C&quot;, 16))
material = c(rep( c(rep(&quot;E1&quot;,4), rep(&quot;E2&quot;,4), rep(&quot;S1&quot;,4), rep(&quot;S2&quot;,4)),3))
bonds = data.frame(substrate, material, strength)
plot(strength~substrate,bonds) </code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>plot(strength~material,bonds)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>It appears material has a greater impact on strength than substrate. We also consider the interaction between substrate and material.</p>
<pre class="r"><code>interaction.plot(bonds$substrate, bonds$material, bonds$strength, xlab=&quot;Substrate&quot;, ylab=&quot;Mean Strength&quot;)</code></pre>
<p><img src="Regression_files/figure-html/interaction2-1.png" width="672" /></p>
<pre class="r"><code>interaction.plot(bonds$material, bonds$substrate, bonds$strength, xlab=&quot;Material&quot;, ylab=&quot;Mean Strength&quot;)</code></pre>
<p><img src="Regression_files/figure-html/interaction2-2.png" width="672" /></p>
<p>The lines are not parallel indicating interaction may be significant.</p>
</div>
<div id="anova-model" class="section level3">
<h3><span class="header-section-number">4.3.2</span> ANOVA Model</h3>
<p>We fit the full 2-way ANOVA model. Any of the fitting methods can be used and produce the same results.</p>
<pre class="r"><code>mod.aov2Full = lm(strength ~ material + substrate + material*substrate, data = bonds)
#mod.aov2Full = lm(strength~material*substrate, data=bonds)
#mod.aov2Full = aov(strength~material*substrate, data=bonds)
#mod.aov2Full = aov(strength ~ material + substrate + material*substrate, data = bonds)
#mod.aov2Full = aov(strength ~ material + substrate + material:substrate, data = bonds)
summary(mod.aov2Full)</code></pre>
<pre><code>## 
## Call:
## lm(formula = strength ~ material + substrate + material * substrate, 
##     data = bonds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64250 -0.08687 -0.00250  0.11000  0.41750 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            1.820e+00  1.116e-01  16.308  &lt; 2e-16 ***
## materialE2             9.450e-01  1.578e-01   5.988 7.22e-07 ***
## materialS1             1.180e+00  1.578e-01   7.477 7.86e-09 ***
## materialS2             1.485e+00  1.578e-01   9.409 3.08e-11 ***
## substrateB            -5.500e-02  1.578e-01  -0.348    0.730    
## substrateC             1.377e+00  1.578e-01   8.728 2.06e-10 ***
## materialE2:substrateB  3.600e-01  2.232e-01   1.613    0.115    
## materialS1:substrateB  1.041e-15  2.232e-01   0.000    1.000    
## materialS2:substrateB  1.700e-01  2.232e-01   0.762    0.451    
## materialE2:substrateC -2.310e+00  2.232e-01 -10.349 2.46e-12 ***
## materialS1:substrateC -1.562e+00  2.232e-01  -7.000 3.28e-08 ***
## materialS2:substrateC -1.192e+00  2.232e-01  -5.343 5.25e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2232 on 36 degrees of freedom
## Multiple R-squared:  0.907,  Adjusted R-squared:  0.8786 
## F-statistic: 31.93 on 11 and 36 DF,  p-value: 2.848e-15</code></pre>
<pre class="r"><code>anova(mod.aov2Full)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: strength
##                    Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## material            3 8.7587 2.91957  58.605 6.268e-14 ***
## substrate           2 0.1041 0.05206   1.045    0.3621    
## material:substrate  6 8.6333 1.43889  28.883 2.312e-12 ***
## Residuals          36 1.7934 0.04982                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVA output shows substrate may not be significant to the model (it also didn’t appear to be significant in the initial boxplot). However, if we remove substrate we must also remove the interaction term (to maintain a hierarchical model). We test if the coefficients for substrate and interaction terms are 0.</p>
<p>We first compute the F-statistic: <span class="math display">\[F = \frac{[SSE_{Reduced} - SSE_{Full}]/[dfe_{Reduced}-dfe_{Full}]}{MSE_{Full}} = \frac{(0.1041+8.6333)/(2+6)}{0.04982}\]</span> Then, we compare this F-statistic to the F distribution with 8 and 36 degrees of freedom to obtain the p-value.</p>
<pre class="r"><code>Fstat = ((0.1041 + 8.6333)/(2+6)) / 0.04982
pvalue = pf(Fstat, 8,36,lower.tail = FALSE)
pvalue</code></pre>
<pre><code>## [1] 1.140389e-11</code></pre>
<p>The small p-value means we reject the null hypothesis and conclude we cannot drop the subtrate and interaction terms.</p>
</div>
<div id="checking-assumptions-1" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Checking Assumptions</h3>
<p><strong>Homoscedasticity</strong></p>
<p>We check the assumption of homoscedasticity with a plot (we could also use Levene’s test)</p>
<pre class="r"><code>plot(mod.aov2Full, which=1)</code></pre>
<p><img src="Regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>There may be some outliers but not overwhelming evidence of heteroscedasticity.</p>
<p><strong>Normality of Residuals</strong></p>
<p>We also check the distribution of the residuals with a plot (we could use the Shapiro-Wilk test).</p>
<pre class="r"><code>plot(mod.aov2Full, which=2)</code></pre>
<p><img src="Regression_files/figure-html/normality2-1.png" width="672" /></p>
</div>
<div id="making-predictions-1" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Making Predictions</h3>
<p>If we want to determine which combination(s) of material and substrate produce the greatest strenght, we could start by computing all bonding material pairwise confidence intervals.</p>
<pre class="r"><code>mod.aov2Full = aov(strength ~ material + substrate + material:substrate, data = bonds)
TukeyHSD(mod.aov2Full,which = &#39;material&#39;, conf.level = 0.95) #TukeyHSD requires an ANOVA object</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = strength ~ material + substrate + material:substrate, data = bonds)
## 
## $material
##            diff        lwr       upr     p adj
## E2-E1 0.2950000 0.04959085 0.5404091 0.0132102
## S1-E1 0.6591667 0.41375752 0.9045758 0.0000001
## S2-E1 1.1441667 0.89875752 1.3895758 0.0000000
## S1-E2 0.3641667 0.11875752 0.6095758 0.0016623
## S2-E2 0.8491667 0.60375752 1.0945758 0.0000000
## S2-S1 0.4850000 0.23959085 0.7304091 0.0000321</code></pre>
<p>This output tells us there is a significant difference between all bonding materials.</p>
<p>We could also plot the mean of each combination with their standard errors. Note, since we are using the full model, the predictions for each of the 12 treatments combinations are equal to each of the cell means.</p>
<pre class="r"><code>library(tidyverse)
meanData = bonds %&gt;% group_by(substrate, material) %&gt;% summarise(strength = mean(strength))
sigma = sqrt(.04982)
se = sigma/sqrt(4) 
pd = position_dodge(0.1) 
ggplot(meanData, aes(x=substrate, y=strength, colour=material, group=material)) + 
  geom_errorbar(aes(ymin=strength-se, ymax=strength+se), colour=&quot;black&quot;, width=.25, position=pd) +
  geom_line(position=pd) +
  geom_point(position=pd, size=4, shape=21, fill=&quot;white&quot;)</code></pre>
<p><img src="Regression_files/figure-html/meanSEPlot-1.png" width="672" /></p>
<p>To obtain the greatest strength we should use material S2 with any subtrate.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
