---
title: "Classification"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(car)
```

# Overview
What is classification, checking model performance - accuracy, ROC, precision/recall, etc., some methods

# Logistic Regression
Logistic regression is used for binary classification i.e. when there are 2 possible predictor classes.

Logistic regression assumes a linear relationship between the predictors variables and the log-odds of the event Y=1:
$$log_b\frac{p}{1-p} = \beta_0 + \beta_1x_1 + \beta_2x_2$$
where $p = P(Y=1).$  We can solve for $p$:
$$p = \frac{b^{\beta_0 + \beta_1x_1 + \beta_2x_2}}{b^{\beta_0 + \beta_1x_1 + \beta_2x_2}+1} = \frac{1}{1+b^{-(\beta_0 + \beta_1x_1 + \beta_2x_2)}}$$

Generally $e$ is chosen for the base $b$, though other bases are possible.  The regression coefficients are generally found with a maxium likelihood estimation that requires an iterative process.

**Assumptions:** logistic regression requires the observations to be independent, little to no multicollinearity among independent variables, and linearity of independent variables and log odds.


## Logistic Regression: Income Example
We want to predict if an individual will earn more than $50K using various predictors.  This example was inspired by [Logistic Regression Income Data](http://r-statistics.co/Logistic-Regression-With-R.html)

### Train/Test Set
Load data:
```{r}
incomeData <- read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', 
                    sep = ',', fill = F, strip.white = T)
colnames(incomeData) <- c('age', 'workclass', 'fnlwgt', 'education', 
                     'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 
                     'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income')
incomeData$IncomeAbove50 = as.numeric(incomeData$income == '>50K')
table(incomeData$income)
```
We see there are significantly more cases of <=50K than >50K.  Because of this class bias, we will select our training data proportionately from each of these classes.
```{r}
dataAbove = incomeData %>% filter(IncomeAbove50 == 1)
dataBelow = incomeData %>% filter(IncomeAbove50 == 0)
set.seed(123)
dataAbove_trainingRows = sample(1:nrow(dataAbove), 0.75*nrow(dataAbove))
dataBelow_trainingRows = sample(1:nrow(dataBelow), 0.75*nrow(dataBelow))
trainAbove = dataAbove[dataAbove_trainingRows,]
trainBelow = dataBelow[dataBelow_trainingRows,]
train = rbind(trainAbove, trainBelow)

testAbove = dataAbove[-dataAbove_trainingRows,]
testBelow = dataBelow[-dataBelow_trainingRows,]
test = rbind(testAbove, testBelow)

table(train$income)
rm(dataAbove, dataBelow, dataAbove_trainingRows, dataBelow_trainingRows, trainAbove, trainBelow, testAbove, testBelow)
```
We have a similar proportion of >50K to <=50K in our training data as in the full data set.


### Fit and Test Logistic Model
I chose variables I thought would predict *income*, however different variable choices or variable selection methods could be used.
```{r}
mod.logit = glm(IncomeAbove50 ~ age + workclass + education_num + sex + hours_per_week, data=train, family='binomial')
summary(mod.logit)
```

Predict for the test data
```{r}
predictedProbability = predict(mod.logit, test, type='response')
test$PredictedIncomeAbove50 = ifelse(predictedProbability > 0.5,1,0)
confusionMatrix(factor(test$PredictedIncomeAbove50), factor(test$IncomeAbove50))
```
Our model predicts our test data with 81% accuracy.  Some other interesting metrics: 

* Sensitivity: also called recall or true positive rate.  Equal to $\frac{TP}{TP + FN}$ where $FN$ (Type II error) occurs when the model predicts a true value to be false  
* Specificity: also called true negative rate.  Equal to $\frac{TN}{TN + FP}$ where $FP$ (Type I error) occurs when the model predicts a false value to be true  

Generally raising one of the metrics lowers the other (trade-off), depending on the context of the data will determine which is more important.

The model prediction returns a probability of each observation having an income greater than 50K.  Here, we classified observations with a probability > 0.5 to have an income greater than 50K, however different threshold values are possible.  

### ROC Curve

### Model Assumptions
We can check for multicollinearity among our independent variables by computing the variance inflation factor (VIF).  All the variables should have a VIF less than 10 but ideally less than 5.

```{r}
vif(mod.logit)
```



# K-Nearest Neighbors (KNN)
KNN works by considering the K training data points that are closest (typically using Euclidean distance) to the test observation.  Then, the test observation is predicted to be in the same class as the majority of the closest training points.  In the event of a tie, one of the classes is randomly chosen.  It is necessary to normalize (generally min-max normalization instead of standardization) the data first since we will be calculating distances.

## KNN: Iris Example
Normalize the iris data and split into a training and test set
```{r}
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
set.seed(123)
irisNormalized = as.data.frame(lapply(iris[,c(1,2,3,4)], normalize))
training_ind = sample(1:nrow(iris), 0.75 * nrow(iris))
train = irisNormalized[training_ind,]
test = irisNormalized[-training_ind,]
train_class = iris[training_ind,5]
test_class = iris[-training_ind,5]
```

Now run knn function
```{r}
library(class)
test_prediction = knn(train, test, cl=train_class, k=3)
```

Calculate accuracy of test data
```{r}
table(test_prediction, test_class)
mean(test_prediction == test_class)*100 #accuracy
```

Try a larger value of k (consider more neighbors)
```{r}
test_prediction = knn(train, test, cl=train_class, k=11)
table(test_prediction, test_class)
mean(test_prediction == test_class)*100 #accuracy
```
In this case, a larger k increased the accuracy of the model, however this was inconsistent across different splits of the training and test sets.  We could use cross-validation to test different values of k (and different train/test sets) to determine the optimal number of neighbors to use.

In general, a smaller value of k means noise in the data will have a higher influence on the boundary causing the boundary to be more flexible.  A larger value of k will have smoother decision boundaries which may not pick up on some boundaries of the data, and is more computationally expensive.

A general rule is to choose $k=\sqrt{N}$ where N is the number of samples in the training data.

# Naive Bayes
Bayes theorem is given as:
$$p(C_k|\textbf{x}) = \frac{p(C_k)p(\textbf{x}|C_k)}{p(\textbf{x})}$$ 
where \textbf{x} represents the features in the data, $C_k$ is a possible class, and $p(C_k|\textbf{x})$ is the probability of belonging to a class given the independent variables defined by \textbf{x}.  This is sometimes rewritten as 
$$\text{posterior} = \frac{\text{prior}\times \text{likelihood}}{\text{evidence}}$$

The denominator of the fraction is constant, and the numerator is equivalent to the joint probability model $p(C_k,x_1,...,x_n)$.  

If we assume all features in \textbf{x} are mutually independent (this assumption is what makes it *naive*), we can rewrite Bayes theorem as 
$$p(C_k|x_1,...,x_n) \propto p(C_k) \prod_{i=1}^n p(x_i|C_k)$$ 

## Naive Bayes Classifier
Assign a class label $\hat{y} = C_k$ for some k where
$$\hat{y} = \text{argmax}_{k \in \{1,...,K\}}p(C_k)\prod_{i=1}^n p(x_i|C_k)$$

## Naive Bayes: Titanic Example  
Naive Bayes is good to use when all the predictors are categorical, but can also be used on continuous predictors.  It also works well when there are more than 2 classes to predict, unlike other classification methods (e.g. logistic regression).  This example was inspired by [Naive Bayes Blog Post](https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/)

We try to predict if a passenger survived on the Titanic given the predictors: Class (1st, 2nd, 3rd, crew), Sex (M,F), and Age (child, adult).
```{r }
data("Titanic")
Titanic = as.data.frame(Titanic)
#Data currently summary data, expand for analysis
repeating_sequence=rep.int(seq_len(nrow(Titanic)), Titanic$Freq)
Titanic_df=Titanic[repeating_sequence,]
Titanic_df = Titanic_df %>% select(-Freq)
```
Fit Naive Bayes model
```{r}
library(e1071)
mod.NB = naiveBayes(Survived~., data=Titanic_df)
mod.NB
```
Make predictions and check accuracy
```{r}
Titanic_df$PredictedSurvived = predict(mod.NB, Titanic_df)
table(Titanic_df$PredictedSurvived, Titanic_df$Survived)
mean(Titanic_df$PredictedSurvived==Titanic_df$Survived)
```
The model predicts if a passenger survives with about 77.8% accuracy.

## Laplace Correction
Suppose there was a set of predictors with a 0 probability of one of the outcomes.  For example, suppose none of the crew survived.  Without any correction, our model will always predict a crew member will not survive, leaving no chance for an unforeseen circumstance.

```{r, warning=F}
missingData = unique(Titanic_df %>% filter(Class=='Crew', Survived=='Yes') %>% select(Class:Survived))
Titanic_df_Less = Titanic_df %>% select(Class:Survived) %>% anti_join(missingData, by = c('Class', 'Sex', 'Age', 'Survived'))
mod.NBNoLaplace = naiveBayes(Survived ~ ., data=Titanic_df_Less, laplace=0)
print(missingData[1,])
predict(mod.NBNoLaplace, missingData[1,], type="raw", threshold=0)
```
 We introduce the Laplace correction, which adds 1 (or whatever defined value) to all the classes.  This forces all joint probabilities to be greater than 0 and allows a (sometimes small) possibility for all outcomes.

```{r, warning=F}
mod.NBLaplace = naiveBayes(Survived ~ ., data=Titanic_df_Less, laplace=1)
predict(mod.NBLaplace, missingData[1,], type="raw", threshold=0)
```








# Classification Trees