<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Clustering</title>

<script src="site_libs/header-attrs-2.6/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Regression.html">Regression</a>
</li>
<li>
  <a href="Classification.html">Classification</a>
</li>
<li>
  <a href="Clustering.html">Clustering</a>
</li>
<li>
  <a href="Databases.html">Databases</a>
</li>
<li>
  <a href="CloudComputing.html">Cloud Computing</a>
</li>
<li>
  <a href="ShellGit.html">Shell/Git</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Clustering</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#overview"><span class="toc-section-number">1</span> Overview</a>
<ul>
<li><a href="#calculating-distance"><span class="toc-section-number">1.1</span> Calculating Distance</a></li>
<li><a href="#clustering-techniques"><span class="toc-section-number">1.2</span> Clustering Techniques</a></li>
</ul></li>
<li><a href="#hierarchical-clustering"><span class="toc-section-number">2</span> Hierarchical Clustering</a>
<ul>
<li><a href="#linkage"><span class="toc-section-number">2.1</span> Linkage</a></li>
<li><a href="#hierarchical-clustering-iris-example"><span class="toc-section-number">2.2</span> Hierarchical Clustering: Iris Example</a></li>
</ul></li>
<li><a href="#k-means-clustering"><span class="toc-section-number">3</span> K-Means Clustering</a>
<ul>
<li><a href="#k-means-iris-example"><span class="toc-section-number">3.1</span> K-Means: Iris Example</a></li>
<li><a href="#parameters-for-k-means-and-determining-number-of-clusters"><span class="toc-section-number">3.2</span> Parameters for K-Means and Determining Number of Clusters</a></li>
</ul></li>
<li><a href="#principal-component-analysis"><span class="toc-section-number">4</span> Principal Component Analysis</a>
<ul>
<li><a href="#iris-example"><span class="toc-section-number">4.1</span> Iris Example</a></li>
<li><a href="#pca-and-clustering"><span class="toc-section-number">4.2</span> PCA and Clustering</a></li>
</ul></li>
</ul>
</div>

<div id="overview" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Overview</h1>
<p>Clustering looks for observations that are <em>similar</em> to one another and groups them together. It’s an unsupervised technique i.e. there aren’t defined/known groups. It is import to scale the data before analysis. There are different ways to calculate distance between observations, and methods to determine how many groups to have.</p>
<div id="calculating-distance" class="section level2" number="1.1">
<h2 number="1.1"><span class="header-section-number">1.1</span> Calculating Distance</h2>
<p>Consider 3 points on a grid: (1,4), (3,2), (7,8) and calculate the distance between them</p>
<pre class="r"><code>points = data.frame(x=c(1,3,7), y=c(4,2,8))
dist(points) #defaults to euclidean</code></pre>
<pre><code>##          1        2
## 2 2.828427         
## 3 7.211103 7.211103</code></pre>
<pre class="r"><code>dist(points, method=&quot;manhattan&quot;)</code></pre>
<pre><code>##    1  2
## 2  4   
## 3 10 10</code></pre>
<div id="importance-of-scaling" class="section level3" number="1.1.1">
<h3 number="1.1.1"><span class="header-section-number">1.1.1</span> Importance of Scaling</h3>
<p>Consider calculating the distance between 3 height (inches) and weight (pounds) pairs.</p>
<pre class="r"><code>height_weight = data.frame(height = c(65, 68, 60), weight = c(140, 175, 115))
dist(height_weight)</code></pre>
<pre><code>##          1        2
## 2 35.12834         
## 3 25.49510 60.53098</code></pre>
<p>Now scale (normalize) the height and weight pairs to be centered at 0 with standard deviation 1 and calculate distance</p>
<pre class="r"><code>height_weight_scaled = scale(height_weight)
dist(height_weight_scaled)</code></pre>
<pre><code>##          1        2
## 2 1.378276         
## 3 1.489525 2.807431</code></pre>
<p>With the unscaled data, observations 1 and 3 are the closest, while with the scaled data observations 1 and 2 are the closest.</p>
</div>
<div id="calculating-distance-for-categorical-variables" class="section level3" number="1.1.2">
<h3 number="1.1.2"><span class="header-section-number">1.1.2</span> Calculating Distance for Categorical Variables</h3>
<p>First convert to dummy variables</p>
<pre class="r"><code>college_gender = data.frame(college = as.factor(c(&#39;ASC&#39;, &#39;ASC&#39;, &#39;BUS&#39;, &#39;BUS&#39;, &#39;BUS&#39;, &#39;ENG&#39;)), gender = as.factor(c(&#39;female&#39;,&#39;male&#39;,&#39;male&#39;,&#39;male&#39;,&#39;female&#39;,&#39;male&#39;)))
#install.packages(&#39;dummies&#39;)
library(dummies)
college_gender_dummy = dummy.data.frame(college_gender)
print(college_gender)</code></pre>
<pre><code>##   college gender
## 1     ASC female
## 2     ASC   male
## 3     BUS   male
## 4     BUS   male
## 5     BUS female
## 6     ENG   male</code></pre>
<pre class="r"><code>print(college_gender_dummy)</code></pre>
<pre><code>##   collegeASC collegeBUS collegeENG genderfemale gendermale
## 1          1          0          0            1          0
## 2          1          0          0            0          1
## 3          0          1          0            0          1
## 4          0          1          0            0          1
## 5          0          1          0            1          0
## 6          0          0          1            0          1</code></pre>
<p>This is just one example of creating dummy variables in R</p>
<p>Then calculate distance</p>
<pre class="r"><code>dist(college_gender_dummy, method=&#39;binary&#39;)</code></pre>
<pre><code>##           1         2         3         4         5
## 2 0.6666667                                        
## 3 1.0000000 0.6666667                              
## 4 1.0000000 0.6666667 0.0000000                    
## 5 0.6666667 1.0000000 0.6666667 0.6666667          
## 6 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000</code></pre>
</div>
</div>
<div id="clustering-techniques" class="section level2" number="1.2">
<h2 number="1.2"><span class="header-section-number">1.2</span> Clustering Techniques</h2>
<p>Some popular clustering techniques are: Hierarchical, K-means, Partition Around Medoids (PAM), and Principal Component Analysis (PCA)</p>
</div>
</div>
<div id="hierarchical-clustering" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Hierarchical Clustering</h1>
<p>Generally agglomerative (“bottom-up”): each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy</p>
<ul>
<li>Put each data point in its own cluster<br />
</li>
<li>Identify the closest (using linkage criteria) two clusters and combine them into one cluster<br />
</li>
<li>Repeat the above step until all the data points are in a single cluster</li>
</ul>
<p>Alternatively divisive (“top-down”): all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy</p>
<div id="linkage" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> Linkage</h2>
<p>Linkage determines the distance between sets of observations as a function of the pairwise distances between observations<br />
Complete/single/average = <span class="math inline">\(max/min/mean \{d(a,b) : a \in A, b \in B\}\)</span></p>
<ul>
<li>Complete/average tends to produce balanced trees while single produces less balanced</li>
</ul>
</div>
<div id="hierarchical-clustering-iris-example" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> Hierarchical Clustering: Iris Example</h2>
<pre class="r"><code>head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code>iris_dist = dist(iris[,1:2]) #defaults to euclidean distance
clusters = hclust(iris_dist) #defaults to complete linkage
plot(clusters, hang=-1, cex=0.5)</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>iris$cluster = cutree(clusters,k=3)
library(ggplot2)
ggplot(iris, aes(Sepal.Length, Sepal.Width, color=as.factor(cluster), shape=Species)) + geom_point(size=2) + theme(legend.title = element_blank())</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Here we knew there were 3 species of flower, so it was easy to choose the number of clusters (k). We could alternatively cut at a height by specifying h in cutree. With complete linkage this tells us the maximum distance to all other members in a cluster is less than h</p>
</div>
</div>
<div id="k-means-clustering" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> K-Means Clustering</h1>
<ul>
<li>Choose number of clusters K</li>
<li>Randomly generate/select K centroids and assign each data point to the nearest centroid</li>
<li>Recalculate centroids and reassign data to nearest centroid until the cluster assignments don’t change</li>
</ul>
<div id="k-means-iris-example" class="section level2" number="3.1">
<h2 number="3.1"><span class="header-section-number">3.1</span> K-Means: Iris Example</h2>
<pre class="r"><code>model_km = kmeans(iris[,1:2], centers=3)
iris$cluster = model_km$cluster
library(ggplot2)
ggplot(iris, aes(Sepal.Length, Sepal.Width, color=as.factor(cluster), shape=Species)) + geom_point(size=2) + theme(legend.title = element_blank())</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="parameters-for-k-means-and-determining-number-of-clusters" class="section level2 tabset tabset-fade tabset-pills" number="3.2">
<h2 class="tabset tabset-fade tabset-pills" number="3.2"><span class="header-section-number">3.2</span> Parameters for K-Means and Determining Number of Clusters</h2>
<ul>
<li>nstart: how many random starts should be done? Run algorithm nstart times and choose the result with the lowest total within sum of squares. Default = 1 but larger value is generally recommended.</li>
<li>iter.max: maximum number of iterations. Default is 10, change if convergence takes longer.</li>
</ul>
<div id="elbow-scree-plot" class="section level3" number="3.2.1">
<h3 number="3.2.1"><span class="header-section-number">3.2.1</span> Elbow (Scree) Plot</h3>
<p>Run many models with varying value of k, calculate total within-cluster sum of squares, find where ‘elbow’ occurs to determine optimal k to use</p>
<pre class="r"><code>library(purrr)
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss &lt;- map_dbl(1:10,  function(k){
  model &lt;- kmeans(x = iris[,1:2], centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df &lt;- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="silhouette-analysis" class="section level3" number="3.2.2">
<h3 number="3.2.2"><span class="header-section-number">3.2.2</span> Silhouette Analysis</h3>
<p>Calculate how similar each observation is with the cluster it’s assigned relative to other clusters. Ranges from -1 (observations may be assigned to the wrong cluster) to 1 (observation is well matched to the assigned cluster). A value of 0 suggests the observation is borderline matched between two clusters</p>
<pre class="r"><code>library(cluster)
# Use map_dbl to run many models with varying value of k
sil_width = map_dbl(2:10,  function(k){
  model = kmeans(x = iris[,1:2], centers = k)
  ss = silhouette(model$cluster, dist(iris[,1:2]))
  mean(ss[,3]) #sil_width
})

# Generate a data frame containing both k and sil_width
sil_df = data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>K=2 has the largest silhouette score, indicating we should use 2 clusters (though recall we know there are actually 3 species).</p>
</div>
</div>
</div>
<div id="principal-component-analysis" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Principal Component Analysis</h1>
<p>PCA finds structure in features and aids in visualization</p>
<ul>
<li>Find linear combination of variables to create principal components</li>
<li>Maintain most variance in the data (first component has largest possible variance)</li>
<li>Principal components are uncorrelated -&gt; orthogonal
<ul>
<li>Resulting vectors are an uncorrelated orthogonal basis set</li>
</ul></li>
</ul>
<p>Same as eigenvalue decomposition of <span class="math inline">\(X^TX\)</span> (covariance matrix - estimates how each variable relates to another, may differ by a constant) and singular value decomposition of <span class="math inline">\(X\)</span> (note the right singular vectors of <span class="math inline">\(X\)</span> are the eigenvectors of <span class="math inline">\(X^TX\)</span>.</p>
<p>Important to scale/normalize data first, especially if input variables use different units of measurement or the input variables have significantly different variances. It seems the general consensus is to center the data as well.</p>
<ul>
<li>First principal component is equal to eigenvector with largest eigenvalue. Eigenvalues tell you how much variance there is in the data in the direction of the eigenvector, so the eigenvector with the largest eigenvalue is the direction the data has the most variation.</li>
</ul>
<div id="iris-example" class="section level2" number="4.1">
<h2 number="4.1"><span class="header-section-number">4.1</span> Iris Example</h2>
<p>PCA function</p>
<pre class="r"><code>rm(iris)
pca.iris = prcomp(x=iris[-5], scale=T, center=T)
summary(pca.iris)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.7084 0.9560 0.38309 0.14393
## Proportion of Variance 0.7296 0.2285 0.03669 0.00518
## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000</code></pre>
<p>The first two principal components account for over 95% of the variance.</p>
<p>Which predictors contribute to which principal components</p>
<pre class="r"><code>pca.iris$rotation</code></pre>
<pre><code>##                     PC1         PC2        PC3        PC4
## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971</code></pre>
<p>Sepal.Width has little contribution to PC1 but almost all of PC2</p>
<div id="visualization" class="section level3" number="4.1.1">
<h3 number="4.1.1"><span class="header-section-number">4.1.1</span> Visualization</h3>
<p>Visualizing first two components</p>
<pre class="r"><code>library(ggfortify)
autoplot(pca.iris, data=iris, colour=&#39;Species&#39;, loadings = TRUE, loadings.colour = &#39;blue&#39;, loadings.label = TRUE, loadings.label.size = 3)</code></pre>
<pre><code>## Warning: `select_()` is deprecated as of dplyr 0.7.0.
## Please use `select()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Petal.Width and Petal.Length are in the same direction indicated they are correlated in the original data</p>
<p>Plot how much variation each principal component explains:</p>
<pre class="r"><code>pca.iris.var = pca.iris$sdev^2
pve = pca.iris.var/sum(pca.iris.var) #proportion of variance explained
plot(pve, xlab = &#39;Principal Component&#39;, ylab = &#39;Proportion of Variance Explained&#39;, main=&#39;Scree Plot&#39;, ylim=c(0,1), type=&quot;b&quot;)</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="pca-and-clustering" class="section level2" number="4.2">
<h2 number="4.2"><span class="header-section-number">4.2</span> PCA and Clustering</h2>
<p>Perform K-Means clustering on the first two components. This differs from the previous k-means example because we now consider all 4 data columns for clustering (by consider the principal components) instead of just using the first 2 data columns. Plot data using first two components</p>
<pre class="r"><code>library(ggfortify)
iris_scaled = scale(iris[,-5]) #Scale data first for PCA
iris_scaled = cbind(iris_scaled, iris[,5])
autoplot(kmeans(iris_scaled[,-5],3), data=iris_scaled, frame=T)</code></pre>
<pre><code>## Warning: `group_by_()` is deprecated as of dplyr 0.7.0.
## Please use `group_by()` instead.
## See vignette(&#39;programming&#39;) for more help
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
